# N-gram Language Model Project


<br/>
<br/>

## Prerequisites


<br/>
<br/>

```python
## dependecies


```

## Cleaning the data

<br/>
<br/>

## Running the Code

The main program and user interface is in <i>main.py</i>. It was written to run in Python3.

```bash

python3 main.py

```
<br/>
<br/>

## Initiation

The program will ask for a desire to build a language model and for an integer to designate the size of the <i>grams</i>.

```bash
Would you like to create an n-gram model?: (Y|n)
Y

Input the gram size you would like: 
4
```

This will intiate a model with the requested gram size. The options menu appear with options to perform new tasks. 

```bash
Options:
	1. Build new n-gram model
	2. Load and train model
	3. Genereate text
	4. Print texts to file
	5. Show model statistics
	6. Test Perplexity of text
	7. Exit
```
Building a new model will delete the previous model and will need to be retrained. Loading a text will train the model to be able to generate text. Options 3 and 4 will generate texts. Option 3 will generate text in the terminal, and option 4 will print that to a file name. Option 5 will show top n-grams and vocabulary in the model. Lastly, the perplexity of a text can be calculated using option 7.

<br/>
<br/>

### Training Method

The training method 

<br/>
<br/>

### Prediction Method

The predict method


<br/>
<br/>

### Evaluation Method

The evaluation method 

```python
output:


```
<br/>
<br/>


## Expansion

Potential expansions for 

<br/>
<br/>

## The Data

The data for this project is from a shakespear corpus.
<br/>
<br/>
<br/>
<br/>

## Authors


* **King De Lany** - *Initial work* - [DelanyK](https://github.com/DelanyK)



## Acknowledgments

*This project was from the python for natural language processing course and the University of Stuttgart. 
